{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "*(Delete this cell before release)*\n",
    "\n",
    "STATUS: In-Work\n",
    "\n",
    "REFERENCES:\n",
    "  - Deploying ML as a Web Service: https://docs.microsoft.com/en-us/azure/machine-learning/desktop-workbench/model-management-service-deploy\n",
    "    \n",
    "TODO:\n",
    "  - Create fallback\n",
    "  - Setup IoT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](assets/solutions-microsoft-logo-small.png)\n",
    "<img src=\"assets/ai.jpg\" style=\"height:200px;float:right;vertical-align:text-top\">\n",
    "\n",
    "## Artificial Intelligence on IaaS++\n",
    "\n",
    "This is part 5 of a 7-part workshop. The Jupyter Notbooks we are using are arranged in the same order as the Team Data Science Process: \n",
    "\n",
    "0 - [Introduction and Setup](./0%20-%20Introduction.ipynb)\n",
    "\n",
    "1 - [Business Understanding](./1%20-%20Business%20Understanding.ipynb)\n",
    "\n",
    "2 - [Data Acquisition and Understanding](./2%20-%20Data%20Acquisition%20and%20Understanding.ipynb)\n",
    "\n",
    "3 - [Modeling](./3%20-%20Modeling.ipynb)\n",
    "\n",
    "4 - *(This Module)* [Deployment](./4%20-%20Deployment.ipynb)\n",
    "\n",
    "5 - [Customer Acceptance](./5%20-%20Customer%20Acceptance.ipynb)\n",
    "\n",
    "6 - [Workshop Wrap-up](./6%20-%20Workshop%20Wrap-up.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<p style=\"border-bottom: 3px solid lightgrey;\"></p> \n",
    "\n",
    "<h1><img style=\"float: left; margin: 0px 15px 15px 0px;\" src=\"./assets/check.png\">Phase Four - Deployment</h1>\n",
    "\n",
    "In the previous notebook, we trained an LSTM neural net to predict aircraft engine failure 30 cycles into the future.\n",
    "\n",
    "In this notebook, we will create the artifacts and scripts to deploy the LSTM model into a webservice on Azure. The artifacts include the model files, and test scripts to validate your model can be used to predict future reliability of the engines based on the present operating characteristics.\n",
    "\n",
    "Read the [Documentation Reference here](https://docs.microsoft.com/en-us/azure/machine-learning/team-data-science-process/lifecycle-deployment)\n",
    "\n",
    "**Goal**\n",
    " - Deploy models with a data pipeline to a production or production-like environment for final user acceptance\n",
    "\n",
    "**How to do it**\n",
    "  - Deploy the model and pipeline to a production or production-like environment for application consumption\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<p><img style=\"float: right; margin: 0px 15px 15px 0px;\" src=\"./assets/aml-logo.png\"><b>Using Azure Machine Learning for this Phase:</b></p>\n",
    "\n",
    "<p><img style=\"float: left; margin: 0px 15px 15px 0px;\" src=\"./assets/checkbox.png\">[Deploy models in production](https://docs.microsoft.com/en-us/azure/machine-learning/team-data-science-process/deploy-models-in-production)</p>\n",
    "<p><img style=\"float: left; margin: 0px 15px 15px 0px;\" src=\"./assets/checkbox.png\">[Configure your environment to operationalize](https://docs.microsoft.com/en-us/azure/machine-learning/preview/cli-for-azure-machine-learning#o16n)</p>\n",
    "<p><img style=\"float: left; margin: 0px 15px 15px 0px;\" src=\"./assets/checkbox.png\">[Model management](https://docs.microsoft.com/en-us/azure/machine-learning/preview/deployment-setup-configuration)</p>\n",
    "\n",
    "<p style=\"border-bottom: 1px solid lightgrey;\"></p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<p style=\"border-bottom: 1px solid lightgrey;\"></p> \n",
    "\n",
    "### Lab 4.0 - Model Building & Evaluation\n",
    "\n",
    "<img src=\"assets/checkmark.jpg\" style=\"float:right;vertical-align:text-top\">\n",
    "\n",
    "Using the training and test data sets we constructed in the previous Jupyter notebook, this notebook builds a LSTM network for scenerio described at [Predictive Maintenance Template](https://gallery.cortanaintelligence.com/Collection/Predictive-Maintenance-Template-3) to predict failure in aircraft engines. We will store the model for deployment in an Azure web service which we build in the next Jupyter notebook. We'll start with setting up the environment for this phase:\n",
    "\n",
    "Instructions:\n",
    " 1. Run the cells below one at a time.\n",
    "\n",
    "#### Lab verification\n",
    "<p><img style=\"float: left; margin: 0px 15px 15px 0px;\" src=\"./assets/checkbox.png\">Ensure that each cell runs, and produces the correct prediction.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "# import the libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import shutil\n",
    "from keras.models import model_from_json\n",
    "\n",
    "import h5py\n",
    "\n",
    "# For creating the deployment schema file\n",
    "from azureml.api.schema.dataTypes import DataTypes\n",
    "from azureml.api.schema.sampleDefinition import SampleDefinition\n",
    "from azureml.api.realtime.services import generate_schema\n",
    "\n",
    "# Use the Azure Machine Learning data collector to log various metrics\n",
    "from azureml.logging import get_azureml_logger\n",
    "run_logger = get_azureml_logger()\n",
    "run_logger.log('amlrealworld.predictivemaintenanceforpm.operationalization','true')\n",
    "\n",
    "# For Azure blob storage access\n",
    "from azure.storage.blob import BlockBlobService\n",
    "from azure.storage.blob import PublicAccess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Model storage \n",
    "\n",
    "We will store the model and operationalization artifacts in an Azure Blob Storage Container for easy retrieval to your deployment platform. When you created your Model Management account, it created a corresponding Azure Storage Account. We can use this central location for the common model assets.\n",
    "\n",
    "Instructions for setting up your Azure Storage account are available within this link (https://docs.microsoft.com/en-us/azure/storage/blobs/storage-python-how-to-use-blob-storage). \n",
    "\n",
    "You will need to copy your account name and account key from the _Access Keys_ area in the portal into the following code block. These credentials will be reused in all of this workshop's Jupyter notebooks.\n",
    "\n",
    "We will handle creating the containers and writing the data to these containers for each notebook. Further instructions for using Azure Blob storage with AML Workbench are available (https://github.com/Azure/ViennaDocs/blob/master/Documentation/UsingBlobForStorage.md).\n",
    "\n",
    "You will need to enter the **ACCOUNT_NAME** as well as the **ACCOUNT_KEY** in order to access Azure Blob storage account you have created. This notebook will create and store all the resulting data files in a blob container under this account:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Enter your Azure blob storage details here \n",
    "ACCOUNT_NAME = \"<your blob storage account name>\"\n",
    "\n",
    "# You can find the account key under the _Access Keys_ link in the \n",
    "# [Azure Portal](portal.azure.com) page for your Azure storage container.\n",
    "ACCOUNT_KEY = \"<your blob storage account key>\"\n",
    "\n",
    "#-------------------------------------------------------------------------------------------\n",
    "# The data from the Data Ingestion and Preparation notebook is stored in the sensordata ingestion container.\n",
    "MODEL_CONTAINER = \"pmlstmmodel\" \n",
    "# Connect to your blob service     \n",
    "az_blob_service = BlockBlobService(account_name=ACCOUNT_NAME, account_key=ACCOUNT_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "These variables tell the notebook where to store and load files for building and testing the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# We will store each of these data sets in a local persistance folder\n",
    "SHARE_ROOT = os.environ['AZUREML_NATIVE_SHARE_DIRECTORY']\n",
    "\n",
    "# These file names detail the data files. \n",
    "TEST_DATA = 'PM_test_files.pkl'\n",
    "\n",
    "# We'll serialize the model in json format\n",
    "LSTM_MODEL = 'modellstm.json'\n",
    "\n",
    "# and store the weights in h5\n",
    "MODEL_WEIGHTS = 'modellstm.h5'\n",
    "\n",
    "# and the schema file\n",
    "SCHEMA_FILE = 'service_schema.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Load the test data frame\n",
    "\n",
    "We have previously stored the test data frame in the local persistence directory indicated by the `SHARE_ROOT` variable. We'll use this test data frame to build the model schema to describe the deployment function calls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_pickle(SHARE_ROOT + TEST_DATA)\n",
    "test_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We will need to recreate the feature engineering (creating the sequence features) just as we did in the model building step. We will do this within the webservice so that the service can take the raw sensor data, and return a scored result predicting probability of failure at 30 days (`label1`). \n",
    "\n",
    "When scoring an unseen observation, the model will not know the true labels. Therefore, we create a `score_df` without labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# pick the feature columns \n",
    "# Sequence help order the observations in \"time\"\n",
    "sequence_cols = ['setting1', 'setting2', 'setting3', 'cycle_norm']\n",
    "\n",
    "# key columns group the machines\n",
    "key_cols = ['id', 'cycle']\n",
    "\n",
    "# Labels are what we're predicting.\n",
    "label_cols = ['label1', 'label2', 'RUL']\n",
    "\n",
    "# The scoreing data should not have labels... if we knew the label, \n",
    "# we wouldn'y need to predict.\n",
    "score_df = test_df.drop(label_cols, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Test init() and run() functions\n",
    "\n",
    "The web service requires two functions, an `init()` function that will initialize the web service by loading the model into the service, and a `run()` function that will engineer the features to match the model call structure, and score that data set. We create the functions in here for testing and debugging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def init():\n",
    "    # read in the model file\n",
    "    from keras.models import model_from_json\n",
    "    global loaded_model\n",
    "    \n",
    "    # load json and create model\n",
    "    with open(SHARE_ROOT + LSTM_MODEL, 'r') as json_file:\n",
    "        loaded_model_json = json_file.read()\n",
    "        json_file.close()\n",
    "        loaded_model = model_from_json(loaded_model_json)\n",
    "    \n",
    "    # load weights into new model\n",
    "    loaded_model.load_weights(os.path.join(SHARE_ROOT, MODEL_WEIGHTS))\n",
    "    loaded_model.compile('sgd','mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def run(score_input): \n",
    "    # Create the sequences\n",
    "    sequence_length = 50\n",
    "    sequence_cols = ['setting1', 'setting2', 'setting3', 'cycle_norm']\n",
    "    key_cols = ['id', 'cycle']\n",
    "\n",
    "    # Feature engineering\n",
    "    input_features = score_input.columns.values.tolist()\n",
    "    sensor_cols = [x for x in input_features if x not in set(key_cols)]\n",
    "    sensor_cols = [x for x in sensor_cols if x not in set(sequence_cols)]\n",
    "\n",
    "    # The time is sequenced along\n",
    "    # This may be a silly way to get these column names, but it's relatively clear\n",
    "    sequence_cols.extend(sensor_cols)\n",
    "    \n",
    "    seq_array = [score_input[score_input['id']==id][sequence_cols].values[-sequence_length:] \n",
    "                 for id in score_input['id'].unique() if len(score_input[score_input['id']==id]) >= sequence_length]\n",
    "\n",
    "    seq_array = np.asarray(seq_array).astype(np.float32)\n",
    "    try:\n",
    "        prediction = loaded_model.predict_proba(seq_array)\n",
    "        #print(prediction)\n",
    "        pred = prediction.tolist()\n",
    "        return(pred)\n",
    "    except Exception as e:\n",
    "        return(str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check that there are 100 unique engine IDs: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print(score_df.id.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The webservice test requires an `initialize` of the webservice, then send the entire scoring data set into the model. We expect to get 1 probability prediction for each engine in the scoring data set. Since the `score_df` has 100 machines, we expect 100 probabilities back:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "init()\n",
    "\n",
    "prb=run(score_df)\n",
    "print(prb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Instead we get 93, because 7 of the machines do not have the full 50 cycles available for scoring. If we send a machine with fewer than 50 records we get the following back: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "tst_df=score_df.loc[score_df['id'] == 1]\n",
    "\n",
    "print(tst_df.shape)\n",
    "\n",
    "# Because \n",
    "run(tst_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If we send a complete data set, like machineID == 3, we get a probability back:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "tst_df=score_df.loc[score_df['id'] == 3]\n",
    "\n",
    "print(tst_df.shape)\n",
    "\n",
    "# Because \n",
    "ans=run(tst_df)\n",
    "\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Persist model assets\n",
    "\n",
    "Next we persist the assets we have created for use in operationalization. First we need to define the schema so the webservice knows what the payload data will look like as it comes in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# define the input data frame\n",
    "inputs = {\"score_input\": SampleDefinition(DataTypes.PANDAS, score_df)}\n",
    "\n",
    "json_schema = generate_schema(run_func=run, inputs=inputs, filepath=SCHEMA_FILE)\n",
    "\n",
    "# save the schema file for deployment\n",
    "out = json.dumps(json_schema)\n",
    "with open(SHARE_ROOT + SCHEMA_FILE, 'w') as f:\n",
    "    f.write(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The conda dependencies are defined in this `webservices_conda.yaml` file. This will be used to tell the webservice server which python packages are required to run this web service:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile {SHARE_ROOT}/webservices_conda.yaml\n",
    "\n",
    "# Conda environment specification. The dependencies defined in this file will\n",
    "# be automatically provisioned for managed runs. These include runs against\n",
    "# the localdocker, remotedocker, and cluster compute targets.\n",
    "\n",
    "# Note that this file is NOT used to automatically manage dependencies for the\n",
    "# local compute target. To provision these dependencies locally, run:\n",
    "# conda env update --file conda_dependencies.yml\n",
    "\n",
    "# Details about the Conda environment file format:\n",
    "# https://conda.io/docs/using/envs.html#create-environment-file-by-hand\n",
    "\n",
    "# For managing Spark packages and configuration, see spark_dependencies.yml.\n",
    "\n",
    "name: project_environment\n",
    "channels:\n",
    "- conda-forge\n",
    "- defaults\n",
    "dependencies:\n",
    "  - python=3.5.2\n",
    "  - pip:\n",
    "    - azure-common==1.1.8\n",
    "    - azure-storage==0.36.0\n",
    "    - numpy==1.14.0 \n",
    "    - sklearn\n",
    "    - keras\n",
    "    - tensorflow\n",
    "    - h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The `lstmscore.py` file is python code defining the web service operation. It includes both the `init()` and `run()` functions defined earlier imports the required libraries. These should be nearly identical to the previous defined versions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile {SHARE_ROOT}/lstmscore.py\n",
    "\n",
    "# import the libraries\n",
    "import keras\n",
    "import tensorflow\n",
    "import json\n",
    "import shutil\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def init():\n",
    "    # read in the model file\n",
    "    from keras.models import model_from_json\n",
    "    global loaded_model\n",
    "    \n",
    "    # load json and create model\n",
    "    with open('modellstm.json', 'r') as json_file:\n",
    "        loaded_model_json = json_file.read()\n",
    "        json_file.close()\n",
    "        loaded_model = model_from_json(loaded_model_json)\n",
    "    \n",
    "    # load weights into new model\n",
    "    loaded_model.load_weights(\"modellstm.h5\")\n",
    "    loaded_model.compile('sgd','mse')\n",
    "\n",
    "def run(score_input):\n",
    "    # Create the sequences\n",
    "    sequence_length = 50\n",
    "    sequence_cols = ['setting1', 'setting2', 'setting3', 'cycle_norm']\n",
    "    key_cols = ['id', 'cycle']\n",
    "\n",
    "    input_features = score_input.columns.values.tolist()\n",
    "    sensor_cols = [x for x in input_features if x not in set(key_cols)]\n",
    "    sensor_cols = [x for x in sensor_cols if x not in set(sequence_cols)]\n",
    "\n",
    "    # The time is sequenced along\n",
    "    # This may be a silly way to get these column names, but it's relatively clear\n",
    "    sequence_cols.extend(sensor_cols)\n",
    "    \n",
    "    seq_array = [score_input[score_input['id']==id][sequence_cols].values[-sequence_length:] \n",
    "                 for id in score_input['id'].unique() if len(score_input[score_input['id']==id]) >= sequence_length]\n",
    "\n",
    "    seq_array = np.asarray(seq_array).astype(np.float32)\n",
    "    try:\n",
    "        prediction = loaded_model.predict_proba(seq_array)\n",
    "        \n",
    "        pred = prediction.tolist()\n",
    "        return(pred)\n",
    "    except Exception as e:\n",
    "        return(str(e))\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    init()\n",
    "    run(\"{\\\"score_df\\\": [{\\\"s20\\\": 0.5581395348837184, \\\"s10\\\": 0.0, \\\"s2\\\": 0.5451807228915584, \\\"s21\\\": 0.6618337475835432, \\\"s9\\\": 0.12761374854168395, \\\"s19\\\": 0.0, \\\"s3\\\": 0.31066056245912677, \\\"cycle\\\": 1, \\\"s15\\\": 0.3089649865332831, \\\"s4\\\": 0.2694125590817009, \\\"s1\\\": 0.0, \\\"s11\\\": 0.2083333333333357, \\\"cycle_norm\\\": 0.0, \\\"s13\\\": 0.2205882352941444, \\\"s5\\\": 0.0, \\\"s18\\\": 0.0, \\\"s8\\\": 0.2121212121210192, \\\"s14\\\": 0.1321601816492901, \\\"s6\\\": 1.0, \\\"setting2\\\": 0.75, \\\"setting1\\\": 0.632183908045977, \\\"s12\\\": 0.6460554371002161, \\\"s17\\\": 0.3333333333333357, \\\"s16\\\": 0.0, \\\"id\\\": 1, \\\"setting3\\\": 0.0, \\\"s7\\\": 0.6521739130434696}, {\\\"s20\\\": 0.6821705426356601, \\\"s10\\\": 0.0, \\\"s2\\\": 0.15060240963856586, \\\"s21\\\": 0.6868268434134208, \\\"s9\\\": 0.14668401687158195, \\\"s19\\\": 0.0, \\\"s3\\\": 0.37955090473076325, \\\"cycle\\\": 2, \\\"s15\\\": 0.21315890727203168, \\\"s4\\\": 0.2223160027008788, \\\"s1\\\": 0.0, \\\"s11\\\": 0.38690476190476275, \\\"cycle_norm\\\": 0.002770083102493075, \\\"s13\\\": 0.26470588235270043, \\\"s5\\\": 0.0, \\\"s18\\\": 0.0, \\\"s8\\\": 0.16666666666696983, \\\"s14\\\": 0.20476829394158358, \\\"s6\\\": 1.0, \\\"setting2\\\": 0.25, \\\"setting1\\\": 0.3448275862068965, \\\"s12\\\": 0.7398720682302695, \\\"s17\\\": 0.4166666666666714, \\\"s16\\\": 0.0, \\\"id\\\": 1, \\\"setting3\\\": 0.0, \\\"s7\\\": 0.8051529790660226}, {\\\"s20\\\": 0.7286821705426334, \\\"s10\\\": 0.0, \\\"s2\\\": 0.3765060240963862, \\\"s21\\\": 0.7213476940071786, \\\"s9\\\": 0.15808130664991182, \\\"s19\\\": 0.0, \\\"s3\\\": 0.34663178548071016, \\\"cycle\\\": 3, \\\"s15\\\": 0.4586379376683354, \\\"s4\\\": 0.3222484807562438, \\\"s1\\\": 0.0, \\\"s11\\\": 0.38690476190476275, \\\"cycle_norm\\\": 0.0055401662049861505, \\\"s13\\\": 0.2205882352941444, \\\"s5\\\": 0.0, \\\"s18\\\": 0.0, \\\"s8\\\": 0.22727272727297532, \\\"s14\\\": 0.15564041696769948, \\\"s6\\\": 1.0, \\\"setting2\\\": 0.5833333333333334, \\\"setting1\\\": 0.5172413793103449, \\\"s12\\\": 0.6993603411513902, \\\"s17\\\": 0.4166666666666714, \\\"s16\\\": 0.0, \\\"id\\\": 1, \\\"setting3\\\": 0.0, \\\"s7\\\": 0.6859903381642596}]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We also include a python file `test_service.py` which can test the web service you create. This program will test that there are enough cycles for the webservice to score, and send a single engine set to the web service. The results are printed with the engine ID to simulate how to use the webservice in a production setting.\n",
    "\n",
    "After you create the web service, you will need to edit this file and provide the correct `url` endpoint string before running this program:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile {SHARE_ROOT}/test_service.py\n",
    "\n",
    "import urllib\n",
    "import json \n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# The URL will need to be editted after service create.\n",
    "url = 'http://127.0.0.1:32773/score'\n",
    "\n",
    "## Sequence length will need to match the training sequence length from\n",
    "## 2_model_building_and_evaluation.ipynb\n",
    "sequence_length = 50\n",
    "\n",
    "# We'll read in this data to test the service\n",
    "test_df = pd.read_pickle('PM_test_files.pkl')\n",
    "\n",
    "# Labels are what we're predicting.\n",
    "label_cols = ['label1', 'label2', 'RUL']\n",
    "\n",
    "# The scoreing data should not have labels... if we knew the label, \n",
    "# we wouldn't need to predict.\n",
    "score_df = test_df.drop(label_cols, axis = 1)\n",
    "headers = {'Content-Type':'application/json'}\n",
    "\n",
    "# Now get the machine numbers, for each machine get the \n",
    "# prediction for the label timepoint\n",
    "machineID = score_df['id'].unique()\n",
    "\n",
    "for ind in machineID:\n",
    "    \n",
    "    try:\n",
    "        body = score_df[score_df.id==ind]\n",
    "        print('ID {}: size {}'.format(ind, body.shape))\n",
    "        if body.shape[0] < sequence_length : \n",
    "            print(\"Skipping machineID {} as we need {} records to score and only have {} records.\".format(ind, sequence_length, body.shape[0]))\n",
    "            continue\n",
    "        print('ID {}: {} \\t {}'.format(ind, body.shape, body.tail(sequence_length+ 10).shape))\n",
    "        body = \"{\\\"score_input\\\": \" + \\\n",
    "                    body.tail(sequence_length+10).to_json(orient=\"records\") +\\\n",
    "                    \"}\"\n",
    "        \n",
    "        req = urllib.request.Request(url, str.encode(body), headers) \n",
    "        with urllib.request.urlopen(req) as response:\n",
    "            the_page = response.read()\n",
    "            print('ID {}: {}'.format(ind,the_page))\n",
    "        \n",
    "    except urllib.error.HTTPError as error:\n",
    "        print(\"The request failed with status code {}: \\n{}\".format(error, error.read))\n",
    "\n",
    "        # Print the headers - they include the requert ID and the timestamp, which are useful for debugging the failure\n",
    "        print(error.info())\n",
    "        print(error.reason)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Packaging\n",
    "\n",
    "To move the model artifacts around, we'll zip them into one file. We can then retrieve this file from the persistence shared folder on your DSVM.\n",
    "\n",
    "https://docs.microsoft.com/en-us/azure/machine-learning/preview/how-to-read-write-files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Compress the operationalization assets for easy blob storage transfer\n",
    "# We can remove the persisted data files.\n",
    "# !rm {SHARE_ROOT}/PM*.pkl\n",
    "!rm {SHARE_ROOT}/PM_train_files.pkl\n",
    "!ls {SHARE_ROOT}\n",
    "\n",
    "MODEL_O16N = shutil.make_archive('LSTM_o16n', 'zip', SHARE_ROOT)\n",
    "\n",
    "# Create a new container if necessary, otherwise you can use an existing container.\n",
    "# This command creates the container if it does not already exist. Else it does nothing.\n",
    "az_blob_service.create_container(MODEL_CONTAINER,\n",
    "                                 fail_on_exist=False, \n",
    "                                 public_access=PublicAccess.Container)\n",
    "\n",
    "# Transfer the compressed operationalization assets into the blob container.\n",
    "az_blob_service.create_blob_from_path(MODEL_CONTAINER, \"LSTM_o16n.zip\", str(MODEL_O16N)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Deployment\n",
    "\n",
    "Once the assets are stored, we can download them into a deployment compute context for operationalization on an Azure web service. For this scenario, we will deploy this on our local docker container context.\n",
    "\n",
    "We demonstrate how to setup this web service this through an Azure ML CLI window. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Download the model\n",
    "\n",
    "To download the model we've saved, follow these instructions on a local computer.\n",
    "\n",
    " - Open the Azure Portal\n",
    " - In the left hand pane, click on All resources\n",
    " - Search for the storage account using the name you provided earlier in this notebook.\n",
    " - Choose the storage account from search result list, this will open the storage account panel.\n",
    " - On the storage account panel, choose Blobs\n",
    " - On the Blobs panel choose the container `pmlstmmodel`\n",
    " - Select the file `LSTM_o16n.zip` and on the properties pane for that blob, choose download."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Once downloaded, unzip the file into the directory of your choosing. The zip file contains the deployment assets:\n",
    "\n",
    "- the `lstmscore.py` file which contains functionst to do the model scoring\n",
    "- the `modellstm.json` model definition file\n",
    "- the `modellstm.h5` model weights file\n",
    "- the `service_schema.json` which defines the input data schema\n",
    "- the `webservices_conda.yaml` defining which python packages are required to run this service.\n",
    "- the `test_service.py` script for testing your webservice once deployed.\n",
    "- the `PM_test_df.pkl` test data set used by the `test_service.py` script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Create a Model Management Account\n",
    "\n",
    "<img src=\"notebooks/assets/checkmark.jpg\" style=\"float:right;vertical-align:text-top\">\n",
    "\n",
    "Instructions:\n",
    "1.  On your DSVM terminal connection, type the following commands:\n",
    "\n",
    "`az login`\n",
    "\n",
    "2. You will be given instructions for a web page and code to authorize the connection. Perform those actions. You will be given a login panel, use the Azure Account you started with for this Workshop.\n",
    "\n",
    "3. Next, create an Azure Resource Group to hold all of the assets for this workshop with the following commands, entering the same region (which you can find in the Azure Portal) where you created your DSVM:\n",
    " \n",
    "`az group create --name gpurg --location <ACCOUNT_REGION>`\n",
    "\n",
    "*Note: You can find the correct text for the `<ACCOUNT_REGION>` using the command:*\n",
    "\n",
    "`az account list-locations`\n",
    "\n",
    "4. Now create a Model Management service in your Microsoft Azure account, called `gpumodelmanagement`. (You only need to do this once for any models you would like to deploy.) You will need to supply an **ACCOUNT_REGION** as before from your Azure account. The remaining defaults are acceptable. Run the following code:\n",
    "\n",
    "`az ml account modelmanagement create --location <ACCOUNT_REGION> --resource-group gpurg --name gpumodelmanagement`\n",
    "\n",
    "*Note: If you get an error involving the region, read the error message carefully and select a region that is listed.*\n",
    "\n",
    "*Note: If you get a `ResourceGroupNotFound` error, you may need to set the correct subscription. This is typically only an issue if your Azure login connects to multiple subscritpions.*\n",
    "\n",
    "`az account set -s '<subscription name>'`\n",
    "\n",
    "You can find the `subscription name` or `subscription id` through the (https://portal.azure.com) under the resource group you'd like to use. \n",
    "\n",
    "**TODO: Update to Docker**\n",
    "https://azure.github.io/LearnAI-Bootcamp/lab04.1-managing_models_with_aml/0_README\n",
    "and\n",
    "https://azure.github.io/LearnAI-Bootcamp/lab04.2-deploying_a_scoring_service_to_aks/0_README\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Lab verification\n",
    "<p><img style=\"float: left; margin: 0px 15px 15px 0px;\" src=\"./assets/checkbox.png\">Complete the steps that follow:</p>\n",
    "\n",
    "1. Show the currently active environment:\n",
    "\n",
    "`az ml env show`\n",
    "\n",
    "2. You should get a message that the compute environment is not set. If so, activate the environment with your model management context (created above): \n",
    "\n",
    "`az ml env setup --location <ACCOUNT_REGION> --resource-group gpurg --name gpumodelmanagement`\n",
    "\n",
    "...using the same `<ACCOUNT_REGION>` and `<RESOURCE_GROUP>` from the previous section. It will take a few minutes for this to complete. You can check the status with the command:\n",
    "\n",
    "`az ml env show` \n",
    "\n",
    "3. Then set the current environment:\n",
    "\n",
    "`az ml env set --resource-group gpurg --cluster-name gpumodelmanagement`\n",
    "\n",
    "*Note: if you get an error that it is not complete, wait a few minutes and re-run the command.*\n",
    "\n",
    "3.  When that command completes successfully, check that the environment is now set:\n",
    "\n",
    "`az ml env show` \n",
    " \n",
    "<p style=\"border-bottom: 3px solid lightgrey;\"></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Deploy a web service \n",
    "\n",
    "These commands assume the current directory contains the webservice assets we created in throughout the notebooks in this scenario (at least `lstmscore.py`, `modellstm.json`, `modellstm.h5`, `service_schema.json` and `webservices_conda.yaml`). If not, in the AML CLI window, change to the directory where the zip file was unpacked. \n",
    "\n",
    "The command to create a web service (`<SERVICE_ID>`) with these operationalization assets in the current directory is:\n",
    "\n",
    "`\n",
    "az ml service create realtime -f <filename> -r <TARGET_RUNTIME> -m <MODEL_FILE> -s <SCHEMA_FILE> -n <SERVICE_ID> --cpu 0.1\n",
    "`\n",
    "\n",
    "The default cluster has only 2 nodes with 2 cores each. Some cores are taken for system components. AMLWorkbench asks for 1 core per service. To deploy multiple services into this cluster, we specify the cpu requirement in the service create command as (--cpu 0.1) to request 10% of a core. \n",
    "\n",
    "For this example, we will call our webservice with the `SERVICE_ID` = `lstmwebservice`. The `SERVICE_ID` must be all lowercase, with no spaces. This command should work with your account and the deployment artifacts created in this notebook.\n",
    "\n",
    "`\n",
    "az ml service create realtime -f lstmscore.py -r python -m modellstm.json -m modellstm.h5 -s service_schema.json -c webservices_conda.yaml --cpu 0.1 -n lstmwebservice\n",
    "`\n",
    "\n",
    "This command will take some time to execute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Test your deployment.\n",
    "\n",
    "Once complete, the `az ml service create` command returns sample usage commands to test the service for both PowerShell and the cmd prompt. You can copy and paste this command into the CLI to test the web service. However, since it is only an example with 1 cycle, we know the model can not return a reasonable score. Instead you will see the following error as detailed above:\n",
    "```\n",
    "'Error when checking : expected lstm_1_input to have 3 dimensions, but got array with shape (0, 1)'\n",
    "\n",
    "```\n",
    "We have provided the `test_service.py` python script to send larger payloads to the service. First obtain the webservice endpoint with the following command. `az ml service usage realtime -i lstmwebservice`\n",
    "\n",
    "```\n",
    "> az ml service usage realtime -i lstmwebservice\n",
    "Scoring URL:\n",
    "    http://127.0.0.1:32770/score\n",
    "\n",
    "Headers:\n",
    "    Content-Type: application/json\n",
    "\n",
    "Swagger URL:\n",
    "    http://127.0.0.1:32770/swagger.json\n",
    "\n",
    "Sample CLI command:\n",
    "...\n",
    "```\n",
    "\n",
    "Copy the Scoring URL into the `url` variable in the `test_service.py` script and save the file.\n",
    "\n",
    "From the CLI, you can then run the script with the command and response similar to the following:\n",
    "\n",
    "`python test_service.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Deploy to Edge IoT Device\n",
    "\n",
    "<img src=\"notebooks/assets/checkmark.jpg\" style=\"float:right;vertical-align:text-top\">\n",
    "\n",
    "Instructions:\n",
    "1.  Thing:\n",
    "\n",
    "`Command`\n",
    "\n",
    "**TODO: Update to IoT**\n",
    "  - Set up Linux device as IoT Edge: https://docs.microsoft.com/en-us/azure/iot-edge/tutorial-simulate-device-linux \n",
    "  - ML Edge - https://docs.microsoft.com/en-us/azure/iot-edge/tutorial-deploy-machine-learning \n",
    "  - Examples - https://github.com/Azure/ai-toolkit-iot-edge \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<p style=\"border-bottom: 3px solid lightgrey;\"></p> \n",
    "\n",
    "### Phase 4 wrap-up\n",
    "\n",
    "<img src=\"assets/wrapup.jpg\" style=\"float:right;vertical-align:text-top\">\n",
    "\n",
    "<p>This module covered the Deployment phase of the solution.</p>\n",
    "\n",
    "<p>The Notebooks are arranged in the same order as the Team Data Science Process:</p> \n",
    "\n",
    "0 - [Introduction and Setup](./0%20-%20Introduction.ipynb)\n",
    "\n",
    "1 - [Business Understanding](./1%20-%20Business%20Understanding.ipynb)\n",
    "\n",
    "2 - [Data Acquisition and Understanding](./2%20-%20Data%20Acquisition%20and%20Understanding.ipynb)\n",
    "\n",
    "3 - [Modeling](./3%20-%20Modeling.ipynb)\n",
    "\n",
    "4 - *(This module)* [Deployment](./4%20-%20Deployment.ipynb)\n",
    "\n",
    "5 - *(Proceed to this Notebook Next)* [Customer Acceptance](./5%20-%20Customer%20Acceptance.ipynb)\n",
    "\n",
    "6 - [Workshop Wrap-up](./6%20-%20Workshop%20Wrap-up.ipynb)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
